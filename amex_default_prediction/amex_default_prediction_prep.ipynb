{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d88c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import vaex\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import multinomial, norm\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d9670",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ccee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_4_invalid(df, val=50):\n",
    "    # searches for columns with highest scores of missing/NaN values\n",
    "    \n",
    "    # Gathering missing/NaNs for every column\n",
    "    invalid = []\n",
    "    cols = df.get_column_names()\n",
    "    \n",
    "    for count, col in enumerate(cols):\n",
    "        invalid.append(\n",
    "            df[col]\\\n",
    "            .isna()\\\n",
    "            .sum()\\\n",
    "            .item()\\\n",
    "        )\n",
    "        clear_output(wait=True)\n",
    "        print(f'{count+1}/{len(cols)} columns done.')\n",
    "        \n",
    "    print('All done!')\n",
    "    \n",
    "    # Organizing gathered values in a dataframe\n",
    "    miss = pd.DataFrame(\n",
    "        data=np.array(invalid), \n",
    "        index=cols, \n",
    "        columns=['invalids']\n",
    "    )\\\n",
    "        .sort_values(by='invalids', ascending=False\n",
    "    )\n",
    "    \n",
    "    # Format the count column as a percentage of the original dataframe's length\n",
    "    miss['invalids'] = round(miss['invalids']*100/df.shape[0], 2).astype('str') + '%'\n",
    "    \n",
    "    # Displaying large values\n",
    "    query = miss.loc[\n",
    "            np.ceil((miss['invalids'].str.rstrip('%').astype('float'))) > val\n",
    "    ]\n",
    "    \n",
    "    display(query)\n",
    "    \n",
    "    # Get columns with biggest score (index from filtered dataframe)\n",
    "    bad_cols_list = list(query.index)\n",
    "    \n",
    "    return bad_cols_list\n",
    "\n",
    "\n",
    "def clear(df_in, bad_cols_list):\n",
    "    # Deleting columns comprised mostly by invalid values\n",
    "    df_out = df_in.drop(bad_cols_list)\n",
    "    print(f'Dropped {df_in.shape[1] - df_out.shape[1]} columns. {df_out.shape[1]} left.')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def column_selector(df, original_col):\n",
    "    \n",
    "    all_cols = df.get_column_names()\n",
    "    selection = [agg_col for agg_col in all_cols if agg_col.startswith(original_col)]\n",
    "    \n",
    "    return selection\n",
    "\n",
    "\n",
    "def fix_outliers(df_in):\n",
    "    \"\"\"Find outliers by going through the columns and replacing them with the 10th or 90th percentiles, \n",
    "    depending on their values. The outliers were defined as above or below the expression (mean(+-)3stddev)\n",
    "    for the column.\n",
    "    \n",
    "    Args:\n",
    "        df_in (pd.DataFrame)\n",
    "    Returns:\n",
    "        df_out (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_out = df_in.copy()\n",
    "    for count, (col, dtype) in enumerate(zip(df_out.columns, df_out.dtypes.values)):\n",
    "        if (str(dtype).startswith('float') or str(dtype).startswith('int')):\n",
    "            stddev = df_out[col].std()\n",
    "            mean = df_out[col].mean()\n",
    "            if (df_out[col].min() < (mean-3*stddev)) or (df_out[col].max() > (mean+3*stddev)):\n",
    "                perc_10 = df_out[col].quantile(0.10)\n",
    "                perc_90 = df_out[col].quantile(0.90)\n",
    "                df_out.loc[df_out[col] < (mean-3*stddev), col] = perc_10\n",
    "                df_out.loc[df_out[col] > (mean+3*stddev), col] = perc_90       \n",
    "        clear_output(wait=True)\n",
    "        print(f'{count+1}/{len(df_out.columns)} columns done.')\n",
    "        \n",
    "    print('All done!')          \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def fill_numerical_invalids(df_in, cols=None):\n",
    "    # Skip column in case there are no values to change\n",
    "    if cols is None:\n",
    "        cols = df_in.columns\n",
    "    \n",
    "    df_out = df_in.copy()\n",
    "    for count, col in enumerate(cols):\n",
    "        \n",
    "        if df_out[col].isna().sum() == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f'{count+1}/{len(cols)} columns done.')\n",
    "            continue\n",
    "        # Generate a [scipy] normal distribution from current column's values\n",
    "        norm_dist = norm(\n",
    "            loc=df_out[col].mean(),\n",
    "            scale=df_out[col].std()\n",
    "        )\n",
    "        # Series made of values pulled from said distribution and length equal to the amount of invalids\n",
    "        values = pd.Series(\n",
    "            data=norm.rvs(size=(len(df_out),))\n",
    "        )\n",
    "        # Fill column's missing values with those from previously generated normal distribution\n",
    "        df_out.loc[df_out[col].isna(), col] = values\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(f'{count+1}/{len(cols)} columns done.')\n",
    "    \n",
    "    \n",
    "    print('All done!')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def fill_categorical_invalids(df_in, categoricals):\n",
    "    \"\"\"Fill the input dataframe with values taken from a multinomial bernoulli distribution built from the\n",
    "    column being filled.\n",
    "    \n",
    "    Args:\n",
    "        df_in (pd.DataFrame): input dataframe\n",
    "        categoricals (array): names of categorical columns currently on the dataframe\n",
    "        \n",
    "    Returns:\n",
    "        df_out (pd.DataFrame): output dataframe with no missing/NaN values\n",
    "    \"\"\"\n",
    "    \n",
    "    df_out = df_in.copy()\n",
    "    \n",
    "    for count, col in enumerate(categoricals):\n",
    "        \n",
    "        vc = df_out[col].value_counts()\n",
    "        \n",
    "        # In case there are no missing values, skip the column\n",
    "        if df_out[col].isna().sum() == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f'{count+1}/{len(categoricals)} columns done.')\n",
    "            continue\n",
    "        # Gathering values to set up a multinomial distribution\n",
    "        non_missing_index = [ind for ind in vc.index if ind != 'missing' and str(ind) not in ['nan', 'NaN']]\n",
    "        non_missing_count = [count for count, ind in zip(vc.values, vc.index) if ind != 'miing' and str(ind) not in ['nan', 'NaN']]\n",
    "        non_missing_probabilities = non_missing_count/sum(non_missing_count)\n",
    "        # Multinomial distribution object\n",
    "        dist = multinomial(\n",
    "            n=1,  # Each call to the multinomial distribution takes 1 try only\n",
    "            p=non_missing_probabilities\n",
    "        )\n",
    "\n",
    "        # Column with indexes pulled randomly from the distribution then mapping back to original values\n",
    "        indexes = np.argmax(dist.rvs(size=(len(df_out),)), axis=1)\n",
    "        mapping = dict(\n",
    "                zip(range(len(non_missing_index)), non_missing_index)\n",
    "        )\n",
    "        values = pd.Series(np.array(\n",
    "            [mapping[index] for index in indexes]\n",
    "        ).reshape(-1,))\n",
    "        # Fill missing values with random ones pulled from aforementioned dist;\n",
    "        df_out.loc[df_out[col].isna(), col] = values\n",
    "            \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def cat_datatypes_check(df_in, categorical_features):\n",
    "    \n",
    "    # Shallow copy not to modify input dataframe directly\n",
    "    df_out = df_in.copy()\n",
    "    \n",
    "    # Scan for possible convertions to 'int' and 'str', acceptable types for categorical features\n",
    "    for feature, dtype in zip(categorical_features, df_out[categorical_features].dtypes.values):\n",
    "        if str(dtype).startswith('float'):\n",
    "            df_out[feature] = df_out[feature].astype('int32')\n",
    "        elif str(dtype).startswith('object'):\n",
    "            df_out[feature] = df_out[feature].astype('str')\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28073a50",
   "metadata": {},
   "source": [
    "##### Sequence:\n",
    "- Load\n",
    "- Delete mostly invalid features\n",
    "- Detect outliers and map them to median\n",
    "- Fill rest of the invalid values on numeric columns\n",
    "- Fill rest of the invalid values on categorical columns\n",
    "- Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d0aa5",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fb6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/huseyincot/amex-agg-data-pickle\n",
    "train = pd.read_pickle('train_agg.pkl', compression='gzip')\n",
    "test = pd.read_pickle('test_agg.pkl', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9601b35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (458913, 919).\n",
      "Test data shape: (924621, 918).\n"
     ]
    }
   ],
   "source": [
    "train = vaex.from_pandas(train)\n",
    "test = vaex.from_pandas(test)\n",
    "\n",
    "print(f'Train data shape: {train.shape}.')\n",
    "print(f'Test data shape: {test.shape}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cd350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in zip(train.columns, train.dtypes.values):\n",
    "    if str(dtype).startswith('float'):\n",
    "        train[col] = train[col].astype('float32')\n",
    "    elif str(dtype).startswith('int'):\n",
    "        train[col] = train[col].astype('int32')\n",
    "        \n",
    "for col, dtype in zip(test.columns, test.dtypes.values):\n",
    "    if str(dtype).startswith('float'):\n",
    "        test[col] = test[col].astype('float32')\n",
    "    elif str(dtype).startswith('int'):\n",
    "        train[col] = train[col].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b00b8",
   "metadata": {},
   "source": [
    "# Invalid entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9275da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919/919 columns done.\n",
      "All done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invalids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D_87_std</th>\n",
       "      <td>99.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_87_last</th>\n",
       "      <td>99.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_87_max</th>\n",
       "      <td>99.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_87_min</th>\n",
       "      <td>99.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_87_mean</th>\n",
       "      <td>99.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_88_std</th>\n",
       "      <td>99.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_88_min</th>\n",
       "      <td>99.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_88_max</th>\n",
       "      <td>99.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_88_last</th>\n",
       "      <td>99.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_88_mean</th>\n",
       "      <td>99.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110_std</th>\n",
       "      <td>99.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111_std</th>\n",
       "      <td>99.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_39_std</th>\n",
       "      <td>99.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111_last</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111_max</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111_min</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111_mean</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110_last</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110_max</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110_min</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110_mean</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_39_mean</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_39_min</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_39_last</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_39_max</th>\n",
       "      <td>99.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_108_std</th>\n",
       "      <td>98.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_42_std</th>\n",
       "      <td>98.62%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_42_min</th>\n",
       "      <td>98.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_42_last</th>\n",
       "      <td>98.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_42_max</th>\n",
       "      <td>98.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_42_mean</th>\n",
       "      <td>98.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_73_std</th>\n",
       "      <td>98.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_73_last</th>\n",
       "      <td>98.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_73_mean</th>\n",
       "      <td>98.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_73_min</th>\n",
       "      <td>98.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_73_max</th>\n",
       "      <td>98.26%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           invalids\n",
       "D_87_std      99.9%\n",
       "D_87_last    99.82%\n",
       "D_87_max     99.82%\n",
       "D_87_min     99.82%\n",
       "D_87_mean    99.82%\n",
       "D_88_std     99.57%\n",
       "D_88_min     99.47%\n",
       "D_88_max     99.47%\n",
       "D_88_last    99.47%\n",
       "D_88_mean    99.47%\n",
       "D_110_std    99.15%\n",
       "D_111_std    99.15%\n",
       "B_39_std     99.14%\n",
       "D_111_last   99.11%\n",
       "D_111_max    99.11%\n",
       "D_111_min    99.11%\n",
       "D_111_mean   99.11%\n",
       "D_110_last   99.11%\n",
       "D_110_max    99.11%\n",
       "D_110_min    99.11%\n",
       "D_110_mean   99.11%\n",
       "B_39_mean    99.11%\n",
       "B_39_min     99.11%\n",
       "B_39_last    99.11%\n",
       "B_39_max     99.11%\n",
       "D_108_std    98.67%\n",
       "B_42_std     98.62%\n",
       "B_42_min     98.58%\n",
       "B_42_last    98.58%\n",
       "B_42_max     98.58%\n",
       "B_42_mean    98.58%\n",
       "D_73_std     98.36%\n",
       "D_73_last    98.26%\n",
       "D_73_mean    98.26%\n",
       "D_73_min     98.26%\n",
       "D_73_max     98.26%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 36 columns. 883 left.\n",
      "CPU times: total: 1min 56s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = clear(train, search_4_invalid(train, val=98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042e2185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924621, 882)\n"
     ]
    }
   ],
   "source": [
    "# Delete the same columns from the test set\n",
    "dropped_columns = list(set(test.get_column_names()) - set(train.get_column_names()))\n",
    "test = test.drop(columns=dropped_columns)\n",
    "\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834b3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnn names lists for easier selection later on\n",
    "\n",
    "# All columns\n",
    "all_columns = list(train.get_column_names())\n",
    "# Training features\n",
    "training_features = list(set(all_columns) - set(['target']))\n",
    "# Categorical features (as per https://www.kaggle.com/competitions/amex-default-prediction/data)\n",
    "categorical_features_old = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64',\n",
    "                        'D_66', 'D_68']\n",
    "categorical_features = []\n",
    "\n",
    "for old_cat in categorical_features_old:\n",
    "    categorical_features += column_selector(train, old_cat)\n",
    "\n",
    "# Numerical features\n",
    "numerical_features = list(set(training_features) - set(categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf28e9",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8134e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883/883 columns done.\n",
      "All done!\n",
      "CPU times: total: 47.1 s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Outlier treatment function (returns a pandas dataframe)\n",
    "train = fix_outliers(train.to_pandas_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701268cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882/882 columns done.\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "test = fix_outliers(test.to_pandas_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea4b43",
   "metadata": {},
   "source": [
    "# Filling numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "618dd48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849/849 columns done.\n",
      "All done!\n",
      "CPU times: total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training set\n",
    "train = fill_numerical_invalids(\n",
    "    train, \n",
    "    cols=numerical_features\n",
    ")\n",
    "# Test set\n",
    "test = fill_numerical_invalids(\n",
    "    test,\n",
    "    cols=numerical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845b9b",
   "metadata": {},
   "source": [
    "# Filling categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b44955",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 columns done.\n",
      "CPU times: total: 29.2 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fill the invalid values on categorical features\n",
    "train = fill_categorical_invalids(train, categorical_features)\n",
    "# Make sure dtypes on categorical features are all 'int' or 'str'\n",
    "train = cat_datatypes_check(train, categorical_features)\n",
    "\n",
    "# Same process for test set}\n",
    "test = fill_categorical_invalids(test, categorical_features)\n",
    "test = cat_datatypes_check(test, categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2cfb6",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a1b0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export resulting modified dataset\n",
    "train.to_parquet('train_agg_filtered_2.parquet')\n",
    "test.to_parquet('test_agg_filtered_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b33e7ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>maxi</th>\n",
       "      <th>mini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>R_23_min</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>S_19_min</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>R_18_min</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>R_17_min</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>R_13_min</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>R_14_last</td>\n",
       "      <td>35.187500</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>B_10_max</td>\n",
       "      <td>43.875000</td>\n",
       "      <td>-2.956390e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>B_40_max</td>\n",
       "      <td>45.906250</td>\n",
       "      <td>6.258488e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>R_14_max</td>\n",
       "      <td>57.531250</td>\n",
       "      <td>6.556511e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>D_69_max</td>\n",
       "      <td>64.875000</td>\n",
       "      <td>-3.823663e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>849 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       columns       maxi          mini\n",
       "35    R_23_min   0.003967  0.000000e+00\n",
       "129   S_19_min   0.004066  0.000000e+00\n",
       "348   R_18_min   0.004463  0.000000e+00\n",
       "391   R_17_min   0.006226  0.000000e+00\n",
       "658   R_13_min   0.008453  0.000000e+00\n",
       "..         ...        ...           ...\n",
       "91   R_14_last  35.187500  0.000000e+00\n",
       "349   B_10_max  43.875000 -2.956390e-03\n",
       "263   B_40_max  45.906250  6.258488e-06\n",
       "75    R_14_max  57.531250  6.556511e-07\n",
       "524   D_69_max  64.875000 -3.823663e+00\n",
       "\n",
       "[849 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = []\n",
    "maxim = []\n",
    "minim = []\n",
    "\n",
    "for col, coltype in zip(numerical_features, train[numerical_features].dtypes.values):\n",
    "    if str(coltype).startswith('int') or str(coltype).startswith('float'):\n",
    "        columns.append(col)\n",
    "        maxim.append(train[col].max())\n",
    "        minim.append(train[col].min())\n",
    "\n",
    "data = np.array([columns, maxim, minim]).transpose()\n",
    "df_minmax = pd.DataFrame(data=data, columns=['columns', 'maxi', 'mini'])\n",
    "df_minmax.maxi = df_minmax.maxi.astype('float')\n",
    "df_minmax.mini = df_minmax.mini.astype('float')\n",
    "df_minmax.sort_values(by='maxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f1850d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
